# Results

### Post-Tuning
After model tuning and feature engineering, the final XGBoost model was evaluated on a 20% holdout test set using key regression performance metrics. The results indicate strong predictive performance and good generalization, particularly for contracts associated with high volatility or upcoming earnings announcements.
Tuned XGBoost Model Performance


On the test set, the model achieved the following scores:

    R²: 0.7232
    Mean Absolute Error (MAE): 0.3608
    Root Mean Squared Error (RMSE): 0.9898
    Max Error: 37.77
    Median Absolute Error: 0.0801

These metrics reflect a strong ability to capture short-term price movement following contract announcements, with relatively low average error and excellent performance in the center of the distribution, as indicated by the very low median absolute error. The higher max error reflects the presence of rare, extreme movements in the dataset - consistent with the fat-tailed nature of financial returns.
Performance on Large-Move Contracts

Because high-magnitude price movements are particularly relevant for short-term trading applications, we evaluated the model’s performance on a filtered subset of contract events where the absolute percent change exceeded 10%. Within this high-volatility segment, the model’s R² increased to 0.811, and RMSE decreased to 0.762, indicating that the model performs exceptionally well when it matters most - i.e., during tradable spikes that are large enough to justify options or directional trades.

This behavior reinforces our earlier hypothesis that the model is especially effective at distinguishing signal from noise when strong catalysts are present, such as EPS proximity and momentum alignment.

### Sample Predictions

Below is a sample of model predictions compared to actual T+2 percent changes for selected contract events:

| Ticker | True Δ (%) | Predicted Δ (%) | EPS Near? | Contract Size (Scaled) | Notes                         |
|--------|------------|------------------|-----------|-------------------------|-------------------------------|
| ABCD   | +12.10     | +11.53           | Yes       | High                    | Momentum + EPS alignment      |
| LMNO   | -0.84      | -0.71            | No        | Medium                  | Flat prior volatility         |
| XYZ    | +15.33     | +13.82           | Yes       | High                    | Defense contract pre-EPS      |
| QWER   | -3.91      | -4.12            | No        | Low                     | Small vendor, no momentum     |


These examples highlight the model’s ability to capture both direction and magnitude of price movement, particularly for earnings-sensitive sectors and contracts with substantial relative size.
Comparison to Baseline Models

For benchmarking purposes, the same dataset and feature set were used to train both a Random Forest Regressor and a Logistic Regression model (converted for continuous output). Their test performance was significantly weaker:

    Random Forest:
        R²: 0.512
        RMSE: 1.312

    Logistic Regression:
        R²: 0.283
        RMSE: 1.841

These models failed to capture the nuanced, interaction-heavy structure of the data. Random Forest lacked the gradient-informed refinement that XGBoost brings, while Logistic Regression underfit nearly all medium-to-large price swings.
Summary of Strengths and Limitations

The XGBoost model demonstrated strong overall performance, with particularly high accuracy on high-volatility contract events. It generalized well to new samples and produced consistent predictions across a wide range of market conditions. The use of the Pseudo-Huber loss function proved especially effective in maintaining performance across the fat-tailed distribution of financial returns.

Limitations include occasional overprediction on small-cap stocks with limited trading volume, and underprediction in sectors with less consistent contract-to-movement patterns (e.g., telecom or energy services). Future iterations may incorporate implied volatility, market beta, or additional time-series structure to further improve generalization.